{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  5 12:22:28 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:1A:00.0 Off |                  Off |\n",
      "| 30%   30C    P8    21W / 300W |      2MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:1B:00.0 Off |                  Off |\n",
      "| 49%   74C    P2   285W / 300W |  15338MiB / 49140MiB |     91%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:1F:00.0 Off |                  Off |\n",
      "| 30%   30C    P8    25W / 300W |      2MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:20:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    17W / 300W |      2MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    Off  | 00000000:21:00.0 Off |                  Off |\n",
      "| 42%   69C    P2   292W / 300W |  42925MiB / 49140MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    Off  | 00000000:22:00.0 Off |                  Off |\n",
      "| 51%   77C    P2   298W / 300W |  42925MiB / 49140MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000    Off  | 00000000:23:00.0 Off |                  Off |\n",
      "| 48%   74C    P2   296W / 300W |  42925MiB / 49140MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA RTX A6000    Off  | 00000000:24:00.0 Off |                  Off |\n",
      "| 56%   80C    P2   298W / 300W |  42925MiB / 49140MiB |     98%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A     12206      C   python                          11307MiB |\n",
      "|    1   N/A  N/A     13194      C   python                           4029MiB |\n",
      "|    4   N/A  N/A     36861      C   ...arse-retrieval/bin/python    42923MiB |\n",
      "|    5   N/A  N/A     36862      C   ...arse-retrieval/bin/python    42923MiB |\n",
      "|    6   N/A  N/A     36863      C   ...arse-retrieval/bin/python    42923MiB |\n",
      "|    7   N/A  N/A     36864      C   ...arse-retrieval/bin/python    42923MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oogundep/.conda/envs/tevatron/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-05 12:22:32.089594: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-05 12:22:32.252915: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from bean.datasets import TextClassificationLoader\n",
    "from bean.models import AutoModel\n",
    "from bean.evaluate import TextClassificationEvaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2023 12:22:36 - WARNING - datasets.builder - Found cached dataset swahili_news (/home/oogundep/.cache/huggingface/datasets/swahili_news/swahili_news/0.2.0/ed5c9a13b97e0d2864ff1e34bfbd38b2f2c54fea77acffcaef187eb4f13cf8cc)\n",
      "01/05/2023 12:22:37 - WARNING - datasets.builder - Found cached dataset swahili_news (/home/oogundep/.cache/huggingface/datasets/swahili_news/swahili_news/0.2.0/ed5c9a13b97e0d2864ff1e34bfbd38b2f2c54fea77acffcaef187eb4f13cf8cc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 22207\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 7338\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "swahili_news = TextClassificationLoader(dataset_name=\"swahili_news\")\n",
    "train_dataset, dev_dataset, test_dataset = swahili_news.load_data()\n",
    "\n",
    "print(dev_dataset)\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'swahili_news', 'languages_covered': ['swahili'], 'num_labels': 6, 'available_on_huggingface': True, 'dataset_link': 'https://huggingface.co/datasets/swahili_news', 'label2id': {'utumi': 0, 'kitaifa': 1, 'michezo': 2, 'kimataifa': 3, 'burudani': 4, 'afya': 5}, 'text_feature_name': 'text', 'label_feature_name': 'label', 'label_mapped': True}\n"
     ]
    }
   ],
   "source": [
    "dataset_config = swahili_news.get_dataset_metadata(task=\"text_classification\")\n",
    "print(dataset_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Davlan/afro-xlmr-small were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-small and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_class = AutoModel(model_name_or_path=\"Davlan/afro-xlmr-small\", tokenizer_name=\"Davlan/afro-xlmr-small\")\n",
    "model, tokenizer = model_class.classification_model(num_labels=dataset_config['num_labels'], label2id=dataset_config['label2id'], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_configs={\n",
    "    'max_length': 128,\n",
    "    'truncation': True,\n",
    "    'batch_size': 64,\n",
    "    'padding': 'max_length',\n",
    "    'pad_to_max_length': True,\n",
    "    'output_dir': \"models/afroxlmr-base\",\n",
    "    'label_to_id': dataset_config['label2id'],\n",
    "    'text_feature_name': dataset_config['text_feature_name'],\n",
    "    'label_feature_name': dataset_config['label_feature_name'],\n",
    "    'label_mapped': dataset_config['label_mapped'],\n",
    "}\n",
    "evaluator = TextClassificationEvaluate(model=model,tokenizer=tokenizer, **evaluation_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2023 12:24:17 - WARNING - datasets.fingerprint - Parameter 'function'=<bound method TextClassificationEvaluate._tokenize_dataset of <bean.evaluate.evaluate_classifier.TextClassificationEvaluate object at 0x7f521516c610>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "01/05/2023 12:24:17 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/oogundep/.cache/huggingface/datasets/swahili_news/swahili_news/0.2.0/ed5c9a13b97e0d2864ff1e34bfbd38b2f2c54fea77acffcaef187eb4f13cf8cc/cache-629f6fbed82c07cd.arrow\n",
      "01/05/2023 12:24:20 - INFO - bean.evaluate.evaluate_classifier - ***** Running training *****\n",
      "01/05/2023 12:24:20 - INFO - bean.evaluate.evaluate_classifier -   Num examples = 22207\n",
      "01/05/2023 12:24:20 - INFO - bean.evaluate.evaluate_classifier -   Num Epochs = 3\n",
      "01/05/2023 12:24:20 - INFO - bean.evaluate.evaluate_classifier -   Instantaneous batch size per device = 64\n",
      "01/05/2023 12:24:20 - INFO - bean.evaluate.evaluate_classifier -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "01/05/2023 12:24:20 - INFO - bean.evaluate.evaluate_classifier -   Gradient Accumulation steps = 1\n",
      "01/05/2023 12:24:20 - INFO - bean.evaluate.evaluate_classifier -   Total optimization steps = 1041\n",
      " 33%|███▎      | 347/1041 [01:51<03:39,  3.16it/s]01/05/2023 12:26:11 - INFO - bean.evaluate.evaluate_classifier - Loss after epoch: 0 is 0.46895867586135864\n",
      " 67%|██████▋   | 694/1041 [03:41<01:48,  3.19it/s]01/05/2023 12:28:02 - INFO - bean.evaluate.evaluate_classifier - Loss after epoch: 1 is 0.24620991945266724\n",
      " 96%|█████████▌| 1000/1041 [05:19<00:12,  3.21it/s]01/05/2023 12:29:39 - INFO - accelerate.accelerator - Saving current state to models/afroxlmr-base/step_1000\n",
      "01/05/2023 12:29:40 - INFO - accelerate.checkpointing - Model weights saved in models/afroxlmr-base/step_1000/pytorch_model.bin\n",
      "01/05/2023 12:29:41 - INFO - accelerate.checkpointing - Optimizer state saved in models/afroxlmr-base/step_1000/optimizer.bin\n",
      "01/05/2023 12:29:41 - INFO - accelerate.checkpointing - Scheduler state saved in models/afroxlmr-base/step_1000/scheduler.bin\n",
      "01/05/2023 12:29:41 - INFO - accelerate.checkpointing - Random states saved in models/afroxlmr-base/step_1000/random_states_0.pkl\n",
      "100%|██████████| 1041/1041 [05:34<00:00,  3.17it/s]01/05/2023 12:29:54 - INFO - bean.evaluate.evaluate_classifier - Loss after epoch: 2 is 0.205912247300148\n",
      "100%|██████████| 1041/1041 [05:34<00:00,  3.11it/s]\n",
      "01/05/2023 12:29:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/oogundep/.cache/huggingface/datasets/swahili_news/swahili_news/0.2.0/ed5c9a13b97e0d2864ff1e34bfbd38b2f2c54fea77acffcaef187eb4f13cf8cc/cache-e3e70682c2094cac.arrow\n",
      "100%|██████████| 115/115 [00:12<00:00,  9.01it/s]\n"
     ]
    }
   ],
   "source": [
    "finetune_configs = {\n",
    "    'num_train_epochs': 3,\n",
    "    'learning_rate': 2e-5,\n",
    "    'warmup_steps': 0,\n",
    "    'weight_decay': 0.0,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'logging_steps': 100,\n",
    "    'checkpointing_steps': '1000',\n",
    "    'train_batch_size': 64,\n",
    "    'lr_scheduler_type': 'linear',\n",
    "    'num_warmup_steps': 0,\n",
    "    'output_dir': \"models/afroxlmr-base\",\n",
    "    'seed': 0,\n",
    "    'with_tracking': True\n",
    "}\n",
    "scores = evaluator.finetune_evaluate(train_dataset=train_dataset, eval_dataset=test_dataset, config=finetune_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9275006813845734, 'f1': 0.8715676859053234, 'precision': 0.877179924170688, 'recall': 0.866339612756185}\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36872090708e5958aa8910a3f0a7eaba83ef83450d0db8c5434f679138e5fa18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
